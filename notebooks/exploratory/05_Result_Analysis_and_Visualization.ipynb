{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372162ac",
   "metadata": {},
   "source": [
    "# Result Analysis and Visualization\n",
    "\n",
    "## Introduction\n",
    "This notebook analyzes results from the comprehensive ablation study comparing **threshold-based** vs **metric backbone** sparsification strategies. We evaluate performance across:\n",
    "\n",
    "- **Sparsification Methods**: Threshold-based (configurable retention) vs Metric Backbone (fixed retention)\n",
    "- **Edge Metrics**: Jaccard, Adamic-Adar, Approximate Effective Resistance\n",
    "- **GNN Architectures**: GCN, GraphSAGE, GAT\n",
    "- **Datasets**: Cora, PubMed, Flickr\n",
    "- **Seeds**: Multiple seeds for statistical significance\n",
    "\n",
    "**Analysis Objectives:**\n",
    "1. Compare accuracy across sparsification methods and retention ratios\n",
    "2. Analyze effect decomposition (structure vs weighting)\n",
    "3. Measure efficiency (training time, sparsification time, memory)\n",
    "4. Evaluate topology preservation (clustering, connectivity)\n",
    "5. Assess geodesic preservation (shortest path distances)\n",
    "6. Identify Pareto-optimal configurations (accuracy vs efficiency trade-off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated imports\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import yaml\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Robust import: prefer editable install, fallback to path manipulation\n",
    "try:\n",
    "    from src import (\n",
    "        AblationStudy,\n",
    "        DatasetLoader,\n",
    "        GraphSparsifier,\n",
    "        compute_effects,\n",
    "        compute_graph_stats,\n",
    "        retention_to_numeric,\n",
    "        run_ablation_config,\n",
    "        set_global_seed,\n",
    "    )\n",
    "except ImportError:\n",
    "    sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "    from src import (\n",
    "        AblationStudy,\n",
    "        DatasetLoader,\n",
    "        GraphSparsifier,\n",
    "        compute_effects,\n",
    "        compute_graph_stats,\n",
    "        retention_to_numeric,\n",
    "        run_ablation_config,\n",
    "        set_global_seed,\n",
    "    )\n",
    "    print(\"⚠️ Using sys.path fallback. Consider running `pip install -e .` from project root.\")\n",
    "\n",
    "# Matplotlib styling\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# Shared constants\n",
    "METRIC_KEYS = [\"jaccard\", \"adamic_adar\", \"random\"]\n",
    "METRIC_LABELS = [\"Jaccard\", \"Adamic Adar\", \"Random\"]\n",
    "METRIC_COLOR = {\n",
    "    \"Jaccard\": \"#3498db\",\n",
    "    \"Adamic Adar\": \"#e74c3c\",\n",
    "    \"Random\": \"#f1c40f\",\n",
    "}\n",
    "\n",
    "SCENARIO_COLORS = {\n",
    "    \"A: Full + Binary\": \"#2ecc71\",\n",
    "    \"B: Sparse + Binary\": \"#3498db\",\n",
    "    \"C: Full + Weighted\": \"#e74c3c\",\n",
    "    \"D: Sparse + Weighted\": \"#9b59b6\",\n",
    "}\n",
    "\n",
    "markers = {\n",
    "    \"Jaccard\": \"o\",\n",
    "    \"Adamic Adar\": \"s\",\n",
    "    \"Approx ER\": \"D\",\n",
    "    \"Random\": \"x\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2851e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusable Plotting Functions\n",
    "\n",
    "def plot_heatmap_for_dataset_metric(df, dataset, metric, ax=None, figsize=(6, 4)):\n",
    "    \"\"\"Generate a single heatmap for a dataset-metric combination.\"\"\"\n",
    "    metric_df = df[(df[\"Dataset\"] == dataset) & (df[\"Metric\"] == metric)]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        standalone = True\n",
    "    else:\n",
    "        standalone = False\n",
    "    \n",
    "    if metric_df.empty:\n",
    "        ax.set_title(f\"{dataset} - {metric} (no data)\")\n",
    "        ax.axis(\"off\")\n",
    "        return ax\n",
    "    \n",
    "    pivot = metric_df.pivot(index=\"Retention\", columns=\"Scenario\", values=\"Accuracy\").sort_index(ascending=False)\n",
    "    sns.heatmap(pivot, annot=True, fmt=\".1%\", cmap=\"RdYlGn\", \n",
    "                center=float(pivot.values.mean()), ax=ax, \n",
    "                cbar_kws={\"label\": \"Test Accuracy\"})\n",
    "    ax.set_title(f\"{dataset} - {metric}\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Retention Ratio\")\n",
    "    \n",
    "    if standalone:\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_scenarios_for_dataset_metric(df, dataset, metric, ax=None, figsize=(6, 4)):\n",
    "    \"\"\"Generate scenario comparison plot for a dataset-metric combination.\"\"\"\n",
    "    metric_df = df[(df[\"Dataset\"] == dataset) & (df[\"Metric\"] == metric)]\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        standalone = True\n",
    "    else:\n",
    "        standalone = False\n",
    "    \n",
    "    for scenario, color in SCENARIO_COLORS.items():\n",
    "        scenario_data = metric_df[metric_df[\"Scenario\"] == scenario].sort_values(\"Retention\")\n",
    "        ax.plot(scenario_data[\"Retention\"], scenario_data[\"Accuracy\"] * 100, \n",
    "                marker=\"o\", linewidth=2, markersize=6 if standalone else 7, \n",
    "                color=color, label=scenario)\n",
    "    \n",
    "    ax.set_xlabel(\"Retention Ratio\")\n",
    "    ax.set_ylabel(\"Test Accuracy (%)\")\n",
    "    ax.set_title(f\"{dataset} - {metric}\")\n",
    "    ax.legend(loc=\"lower left\", fontsize=8 if standalone else 9)\n",
    "    ax.set_xlim(0.25, 1.0)\n",
    "    ax.invert_xaxis()\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    if standalone:\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_effect_decomposition_for_dataset_metric(effect_df, dataset, metric, ax=None):\n",
    "    \"\"\"Generate effect decomposition plot for a dataset-metric combination.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        standalone = True\n",
    "    else:\n",
    "        standalone = False\n",
    "    \n",
    "    effects_to_plot = [\"Structure Effect (B-A)\", \"Weighting Effect (C-A)\", \"Combined Effect (D-A)\"]\n",
    "    colors = [\"#3498db\", \"#e74c3c\", \"#9b59b6\"]\n",
    "    \n",
    "    metric_effect_df = effect_df[(effect_df[\"Dataset\"] == dataset) & \n",
    "                                  (effect_df[\"Metric\"] == metric) & \n",
    "                                  (effect_df[\"Effect\"].isin(effects_to_plot))]\n",
    "    \n",
    "    for i, effect in enumerate(effects_to_plot):\n",
    "        effect_data = metric_effect_df[metric_effect_df[\"Effect\"] == effect]\n",
    "        ax.plot(effect_data[\"Retention\"], effect_data[\"Value\"] * 100,\n",
    "                marker=\"o\", linewidth=2, markersize=7, color=colors[i], label=effect)\n",
    "    \n",
    "    ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_xlabel(\"Retention Ratio\")\n",
    "    ax.set_ylabel(\"Effect (pp)\")\n",
    "    ax.set_title(f\"{dataset} - {metric}\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_xlim(0.25, 0.95)\n",
    "    ax.invert_xaxis()\n",
    "    \n",
    "    if standalone:\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def save_figure(fig, output_path, dpi=200):\n",
    "    \"\"\"Save figure with consistent settings.\"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(output_path, dpi=dpi, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def generate_and_save_plots(df, datasets, metrics, plot_func, filename_template, \n",
    "                            combined_title, results_root=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Generic function to generate multi-panel plots and save both combined and individual figures.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with results\n",
    "        datasets: List of dataset names\n",
    "        metrics: List of metric names\n",
    "        plot_func: Function(df, dataset, metric, ax) to generate individual plot\n",
    "        filename_template: Template for individual file names (e.g., \"heatmap_{dataset}_{metric}.png\")\n",
    "        combined_title: Title for combined figure\n",
    "        results_root: Root directory for results (uses Path(\"../results\") if None)\n",
    "        **kwargs: Additional arguments passed to plot_func\n",
    "    \"\"\"\n",
    "    if results_root is None:\n",
    "        results_root = Path(\"../results\")\n",
    "    \n",
    "    num_ds = len(datasets)\n",
    "    num_metrics = len(metrics)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_ds, num_metrics, \n",
    "                            figsize=(5 * num_metrics, 4 * num_ds))\n",
    "    axes = np.atleast_2d(axes)\n",
    "    \n",
    "    for row_idx, dataset in enumerate(datasets):\n",
    "        ds_dir = results_root / dataset / \"figures\"\n",
    "        ds_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for col_idx, metric in enumerate(metrics):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            \n",
    "            # Plot on combined figure\n",
    "            plot_func(df, dataset, metric, ax=ax, **kwargs)\n",
    "            \n",
    "            # Create and save individual figure\n",
    "            fig_single, ax_single = plt.subplots(figsize=(6, 4))\n",
    "            plot_func(df, dataset, metric, ax=ax_single, **kwargs)\n",
    "            fig_single.tight_layout()\n",
    "            \n",
    "            filename = filename_template.format(\n",
    "                dataset=dataset, \n",
    "                metric=metric.replace(' ', '_').lower()\n",
    "            )\n",
    "            save_figure(fig_single, ds_dir / filename)\n",
    "            plt.close(fig_single)\n",
    "    \n",
    "    plt.suptitle(combined_title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a22aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Path Management\n",
    "PROJECT_ROOT = Path.cwd().parent.parent\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"config.yaml\"\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "RESULTS_ROOT = Path(\"../results\")\n",
    "\n",
    "def load_config(config_path=CONFIG_PATH):\n",
    "    \"\"\"Load experiment configuration from YAML file.\"\"\"\n",
    "    if config_path.exists():\n",
    "        with open(config_path) as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        print(f\"✓ Loaded config from {config_path}\")\n",
    "        return config\n",
    "    else:\n",
    "        print(f\"⚠️ Config not found at {config_path}, using defaults\")\n",
    "        return {}\n",
    "\n",
    "def get_config_value(config, *keys, default=None):\n",
    "    \"\"\"Safely extract nested config values with fallback.\"\"\"\n",
    "    value = config\n",
    "    for key in keys:\n",
    "        if isinstance(value, dict):\n",
    "            value = value.get(key, {})\n",
    "        else:\n",
    "            return default\n",
    "    return value if value != {} else default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# Configure CPU parallelism and thread usage\n",
    "N_CORES = os.cpu_count() or 1\n",
    "DEFAULT_N_JOBS = max(1, N_CORES - 1)\n",
    "N_JOBS = int(os.environ.get(\"N_JOBS\", DEFAULT_N_JOBS))\n",
    "WORKER_THREADS = int(os.environ.get(\"WORKER_THREADS\", \"1\"))\n",
    "\n",
    "# Limit per-process BLAS/numexpr threads to avoid oversubscription\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(WORKER_THREADS)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(WORKER_THREADS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(WORKER_THREADS)\n",
    "\n",
    "# Parent process threads\n",
    "torch.set_num_threads(WORKER_THREADS)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"CPU cores available: {N_CORES}\")\n",
    "print(f\"Parallel jobs (N_JOBS): {N_JOBS}\")\n",
    "print(f\"Worker threads per job: {WORKER_THREADS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbe3c9",
   "metadata": {},
   "source": [
    "## 1. Run Comprehensive Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb8a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick validation: ensure all target datasets load successfully\n",
    "loader = DatasetLoader(root=str(DATA_ROOT))\n",
    "datasets_to_check = [\"cora\", \"pubmed\", \"flickr\"]\n",
    "\n",
    "check_rows = []\n",
    "for ds in datasets_to_check:\n",
    "    try:\n",
    "        d, nf, nc = loader.get_dataset(ds, DEVICE)\n",
    "        check_rows.append({\n",
    "            \"Dataset\": ds.title(),\n",
    "            \"Nodes\": d.num_nodes,\n",
    "            \"Edges\": int(d.edge_index.shape[1]),\n",
    "            \"Features\": nf,\n",
    "            \"Classes\": nc,\n",
    "            \"Status\": \"OK\",\n",
    "        })\n",
    "    except Exception as e:\n",
    "        check_rows.append({\n",
    "            \"Dataset\": ds.title(),\n",
    "            \"Nodes\": None,\n",
    "            \"Edges\": None,\n",
    "            \"Features\": None,\n",
    "            \"Classes\": None,\n",
    "            \"Status\": f\"ERROR: {type(e).__name__}: {e}\",\n",
    "        })\n",
    "\n",
    "check_df = pd.DataFrame(check_rows)\n",
    "print(\"\\nDataset availability check:\")\n",
    "print(check_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# Extract experiment parameters with fallbacks\n",
    "datasets = get_config_value(config, \"experiment\", \"datasets\", default=[\"Cora\", \"PubMed\", \"Flickr\"])\n",
    "metrics = get_config_value(config, \"experiment\", \"metrics\", default=[\"jaccard\", \"adamic_adar\", \"random\", \"approx_er\"])\n",
    "retentions = get_config_value(config, \"experiment\", \"retentions\", default=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "model_name = get_config_value(config, \"experiment\", \"model\", default=\"gcn\")\n",
    "epochs = get_config_value(config, \"training\", \"epochs\", default=200)\n",
    "patience = get_config_value(config, \"training\", \"patience\", default=20)\n",
    "hidden = get_config_value(config, \"experiment\", \"hidden_channels\", default=64)\n",
    "seed = get_config_value(config, \"experiment\", \"seed\", default=42)\n",
    "\n",
    "print(f\"Experiment Configuration:\")\n",
    "print(f\"  Datasets: {datasets}\")\n",
    "print(f\"  Metrics: {metrics}\")\n",
    "print(f\"  Retentions: {len(retentions)} levels ({min(retentions)}-{max(retentions)})\")\n",
    "print(f\"  Model: {model_name}, Hidden: {hidden}, Epochs: {epochs}, Patience: {patience}, Seed: {seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddccbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define worker function for parallel ablation experiments\n",
    "def run_single_config(dataset_name, metric, retention, data_root, model_name, \n",
    "                      hidden, epochs, patience, seed):\n",
    "    \"\"\"\n",
    "    Run a single ablation configuration (designed for parallel execution).\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        metric: Sparsification metric\n",
    "        retention: Edge retention ratio\n",
    "        data_root: Path to data directory\n",
    "        model_name, hidden, epochs, patience, seed: Training hyperparameters\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with ablation results or None if skipped\n",
    "    \"\"\"\n",
    "    # Skip Flickr for approx_er (too expensive)\n",
    "    if dataset_name.lower() == \"flickr\" and metric in {\"approx_er\", \"approx_effective_resistance\", \"approx-er\"}:\n",
    "        return None\n",
    "    \n",
    "    # Each worker loads its own data to avoid sharing issues\n",
    "    loader = DatasetLoader(root=data_root)\n",
    "    data, num_features, num_classes = loader.get_dataset(dataset_name, device=\"cpu\")\n",
    "    \n",
    "    study = AblationStudy(data, num_features, num_classes, device=\"cpu\")\n",
    "    study.verbose = False\n",
    "    \n",
    "    df = study.run_full_study(\n",
    "        model_name=model_name,\n",
    "        metric=metric,\n",
    "        retention_ratio=retention,\n",
    "        hidden_channels=hidden,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        seed=seed,\n",
    "    )\n",
    "    \n",
    "    df[\"Dataset\"] = dataset_name\n",
    "    df[\"Metric\"] = metric\n",
    "    df[\"Retention\"] = retention\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d2eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute parallel ablation experiments\n",
    "all_configs = [(d, m, r) for d in datasets for m in metrics for r in retentions]\n",
    "print(f\"Running {len(all_configs)} experiment configurations in parallel (N_JOBS={N_JOBS})...\")\n",
    "\n",
    "results = Parallel(n_jobs=N_JOBS, verbose=0, prefer=\"processes\")(\n",
    "    delayed(run_single_config)(d, m, r, str(DATA_ROOT), model_name, hidden, epochs, patience, seed)\n",
    "    for d, m, r in tqdm(all_configs, desc=\"Experiments\")\n",
    ")\n",
    "\n",
    "# Filter None results and combine\n",
    "results = [r for r in results if r is not None]\n",
    "combined = pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "\n",
    "print(f\"\\n✓ Completed {len(results)}/{len(all_configs)} configurations\")\n",
    "print(f\"✓ Combined results shape: {combined.shape}\")\n",
    "\n",
    "# Save results\n",
    "results_file = RESULTS_ROOT / \"ablation_results_all_datasets.csv\"\n",
    "results_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "combined.to_csv(results_file, index=False)\n",
    "print(f\"✓ Results saved to {results_file}\")\n",
    "\n",
    "gc.collect()\n",
    "print(\"✓ Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1173134",
   "metadata": {},
   "source": [
    "## 2. Load Results for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved results for visualization\n",
    "# Try loading comprehensive results first (from notebook 04), then fall back to all_datasets\n",
    "comprehensive_csv = Path(\"../data/ablation_results_comprehensive.csv\")\n",
    "results_csv = RESULTS_ROOT / \"ablation_results_all_datasets.csv\"\n",
    "\n",
    "if comprehensive_csv.exists():\n",
    "    print(f\"Loading comprehensive results from {comprehensive_csv}...\")\n",
    "    df = pd.read_csv(comprehensive_csv)\n",
    "    print(f\"Loaded {len(df)} rows from comprehensive study\")\n",
    "elif results_csv.exists():\n",
    "    print(f\"Loading results from {results_csv}...\")\n",
    "    df = pd.read_csv(results_csv)\n",
    "    # Add SparsificationType column if not present (backward compatibility)\n",
    "    if \"SparsificationType\" not in df.columns:\n",
    "        df[\"SparsificationType\"] = \"Threshold\"\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "else:\n",
    "    print(\"No CSV found - using combined results from memory\")\n",
    "    df = combined.copy()\n",
    "\n",
    "# Normalize column values for consistency\n",
    "df[\"Metric\"] = df[\"Metric\"].str.replace(\"_\", \" \").str.title()\n",
    "df[\"Metric\"] = df[\"Metric\"].str.replace(\"Approx Er\", \"Approx ER\", regex=False)\n",
    "if \"Dataset\" in df.columns:\n",
    "    df[\"Dataset\"] = df[\"Dataset\"].str.title()\n",
    "else:\n",
    "    df[\"Dataset\"] = \"Cora\"  # Default for single-dataset runs\n",
    "\n",
    "# Ensure required columns exist\n",
    "if \"ActualRetention\" not in df.columns:\n",
    "    df[\"ActualRetention\"] = df[\"Retention\"] if \"Retention\" in df.columns else 1.0\n",
    "if \"SparsifySec\" not in df.columns:\n",
    "    df[\"SparsifySec\"] = df[\"PreprocessSec\"] / 2 if \"PreprocessSec\" in df.columns else 0.0\n",
    "if \"WeightSec\" not in df.columns:\n",
    "    df[\"WeightSec\"] = df[\"PreprocessSec\"] / 2 if \"PreprocessSec\" in df.columns else 0.0\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nMemory: {mem.available / 1e9:.1f}GB available ({mem.percent:.0f}% used)\")\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "print(f\"Datasets: {sorted(df['Dataset'].unique())}\")\n",
    "print(f\"Metrics: {sorted(df['Metric'].unique())}\")\n",
    "print(f\"Sparsification Types: {sorted(df['SparsificationType'].unique()) if 'SparsificationType' in df.columns else ['N/A']}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee90ce4",
   "metadata": {},
   "source": [
    "## 3. Heatmap: Test Accuracy by Retention and Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate heatmaps using reusable function\n",
    "unique_datasets = sorted(df[\"Dataset\"].unique())\n",
    "\n",
    "fig = generate_and_save_plots(\n",
    "    df=df,\n",
    "    datasets=unique_datasets,\n",
    "    metrics=METRIC_LABELS,\n",
    "    plot_func=plot_heatmap_for_dataset_metric,\n",
    "    filename_template=\"heatmap_{dataset}_{metric}.png\",\n",
    "    combined_title=\"Test Accuracy Heatmaps per Dataset (Including Random Baseline)\",\n",
    "    results_root=RESULTS_ROOT\n",
    ")\n",
    "\n",
    "# Save combined figure\n",
    "save_figure(fig, RESULTS_ROOT / \"heatmaps_all_datasets.png\")\n",
    "print(f\"✓ Saved combined heatmaps to {RESULTS_ROOT / 'heatmaps_all_datasets.png'}\")\n",
    "print(f\"✓ Saved individual heatmaps to per-dataset folders\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf869455",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "gc.collect()\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"✓ Memory: {mem.available / 1e9:.1f}GB available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12f592",
   "metadata": {},
   "source": [
    "### Interpretation: Heatmaps\n",
    "\n",
    "**What to look for:**\n",
    "- **Scenario A (Full + Binary)**: Baseline performance using all edges with binary weights\n",
    "- **Scenario B (Sparse + Binary)**: Impact of edge removal alone\n",
    "- **Scenario C (Full + Weighted)**: Impact of learned edge weights alone\n",
    "- **Scenario D (Sparse + Weighted)**: Combined effect of sparsification and weighting\n",
    "\n",
    "**Key Observations:**\n",
    "1. **Retention Sweet Spot**: Identify retention ratios where performance remains high (typically 50-70%)\n",
    "2. **Scenario D Performance**: Look for cases where Scenario D (sparse + weighted) matches or exceeds Scenario A (baseline)\n",
    "3. **Metric Differences**: Compare how different sparsification metrics (Jaccard, Adamic-Adar, Random) preserve task performance\n",
    "4. **Dataset Sensitivity**: Note which datasets are more sensitive to sparsification\n",
    "\n",
    "**Note:** If Flickr shows \"no data\" for `approx_er`, this metric was excluded due to computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ae837",
   "metadata": {},
   "source": [
    "## 4. Effect Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9dcf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect decomposition per dataset\n",
    "unique_datasets = sorted(df[\"Dataset\"].unique())\n",
    "\n",
    "effect_rows = []\n",
    "for dataset in unique_datasets:\n",
    "    df_ds = df[df[\"Dataset\"] == dataset]\n",
    "    for metric in df_ds[\"Metric\"].unique():\n",
    "        for retention in sorted(df_ds[\"Retention\"].unique()):\n",
    "            subset = df_ds[(df_ds[\"Metric\"] == metric) & (df_ds[\"Retention\"] == retention)]\n",
    "            effects = compute_effects(subset)\n",
    "            for effect_name, value in effects.items():\n",
    "                effect_rows.append({\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"Metric\": metric,\n",
    "                    \"Retention\": retention,\n",
    "                    \"Effect\": effect_name,\n",
    "                    \"Value\": value,\n",
    "                })\n",
    "\n",
    "effect_df = pd.DataFrame(effect_rows)\n",
    "\n",
    "fig, axes = plt.subplots(len(unique_datasets), len(METRIC_LABELS), figsize=(5 * len(METRIC_LABELS), 5 * len(unique_datasets)))\n",
    "axes = np.atleast_2d(axes)\n",
    "effects_to_plot = [\"Structure Effect (B-A)\", \"Weighting Effect (C-A)\", \"Combined Effect (D-A)\"]\n",
    "colors = [\"#3498db\", \"#e74c3c\", \"#9b59b6\"]\n",
    "\n",
    "for row_idx, dataset in enumerate(unique_datasets):\n",
    "    ds_effect_df = effect_df[effect_df[\"Dataset\"] == dataset]\n",
    "    for col_idx, metric in enumerate(METRIC_LABELS):\n",
    "        metric_effect_df = ds_effect_df[(ds_effect_df[\"Metric\"] == metric) & (ds_effect_df[\"Effect\"].isin(effects_to_plot))]\n",
    "        ax = axes[row_idx, col_idx]\n",
    "        for i, effect in enumerate(effects_to_plot):\n",
    "            effect_data = metric_effect_df[metric_effect_df[\"Effect\"] == effect]\n",
    "            ax.plot(\n",
    "                effect_data[\"Retention\"],\n",
    "                effect_data[\"Value\"] * 100,\n",
    "                marker=\"o\",\n",
    "                linewidth=2,\n",
    "                markersize=7,\n",
    "                color=colors[i],\n",
    "                label=effect,\n",
    "            )\n",
    "        ax.axhline(0, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "        ax.set_xlabel(\"Retention Ratio\")\n",
    "        ax.set_ylabel(\"Effect (pp)\")\n",
    "        ax.set_title(f\"{dataset} - {metric}\")\n",
    "        ax.legend(fontsize=9)\n",
    "        ax.set_xlim(0.25, 0.95)\n",
    "        ax.invert_xaxis()\n",
    "\n",
    "plt.suptitle(\"Effect Decomposition per Dataset\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0c3dd",
   "metadata": {},
   "source": [
    "### Interpretation: Effect Decomposition\n",
    "\n",
    "**Understanding the Effects:**\n",
    "\n",
    "This analysis decomposes the performance impact into three components:\n",
    "\n",
    "1. **Structure Effect (B-A)**: \n",
    "   - Impact of removing edges (sparsification) with binary weights\n",
    "   - **Negative values** indicate performance loss from topology reduction\n",
    "   - Shows how much the graph structure alone matters\n",
    "\n",
    "2. **Weighting Effect (C-A)**:\n",
    "   - Impact of learned edge weights on the full graph\n",
    "   - **Positive values** indicate that weighting helps even without sparsification\n",
    "   - Reveals whether the model can learn useful edge importance\n",
    "\n",
    "3. **Combined Effect (D-A)**:\n",
    "   - Total impact of sparse + weighted approach vs baseline\n",
    "   - **Near-zero or positive** values indicate successful compensation\n",
    "   - Shows if weighting can overcome sparsification losses\n",
    "\n",
    "**Key Insights:**\n",
    "- **Synergy**: If |Combined| < |Structure| + |Weighting|, the two effects work together\n",
    "- **Compensation**: When Combined > Structure, weighting successfully offsets edge removal\n",
    "- **Metric Comparison**: Different sparsification metrics may have different effect profiles\n",
    "- **Dataset Differences**: Some datasets may benefit more from weighting than others\n",
    "\n",
    "**Critical Retention Regions:**\n",
    "- High retention (70-90%): Small structure effect, weighting may provide marginal gains\n",
    "- Medium retention (40-70%): Trade-off region where weighting becomes critical\n",
    "- Low retention (<40%): Structure loss may be too large to compensate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b3d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save effect decomposition results\n",
    "effect_csv = RESULTS_ROOT / \"effect_decomposition_all_datasets.csv\"\n",
    "effect_df.to_csv(effect_csv, index=False)\n",
    "print(f\"✓ Effect decomposition saved to {effect_csv}\")\n",
    "\n",
    "plt.close('all')\n",
    "gc.collect()\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"✓ Memory: {mem.available / 1e9:.1f}GB available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c5146",
   "metadata": {},
   "source": [
    "## 5. Scenario Comparison Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scenario comparison plots using reusable function\n",
    "unique_datasets = sorted(df[\"Dataset\"].unique())\n",
    "\n",
    "fig = generate_and_save_plots(\n",
    "    df=df,\n",
    "    datasets=unique_datasets,\n",
    "    metrics=METRIC_LABELS,\n",
    "    plot_func=plot_scenarios_for_dataset_metric,\n",
    "    filename_template=\"scenarios_{dataset}_{metric}.png\",\n",
    "    combined_title=\"Test Accuracy vs Retention Ratio per Dataset\",\n",
    "    results_root=RESULTS_ROOT\n",
    ")\n",
    "\n",
    "# Save combined figure\n",
    "save_figure(fig, RESULTS_ROOT / \"scenarios_all_datasets.png\")\n",
    "print(f\"✓ Saved combined scenario plots to {RESULTS_ROOT / 'scenarios_all_datasets.png'}\")\n",
    "print(f\"✓ Saved individual scenario plots to per-dataset folders\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce577a",
   "metadata": {},
   "source": [
    "### Interpretation: Scenario Comparison\n",
    "\n",
    "**Understanding the Lines:**\n",
    "- Each line represents one of the four scenarios (A, B, C, D)\n",
    "- X-axis shows edge retention ratio (1.0 = full graph, 0.3 = 30% edges retained)\n",
    "- Y-axis shows test accuracy as percentage\n",
    "\n",
    "**Key Patterns to Observe:**\n",
    "1. **Baseline Degradation (Scenario A)**: How much does performance drop as we reduce edges with no compensation?\n",
    "2. **Weighting Benefit (A vs C)**: Does adding learned weights help even with the full graph?\n",
    "3. **Sparsification Tolerance**: At what retention ratio does performance significantly degrade?\n",
    "4. **Optimal Strategy (Scenario D)**: Can sparse + weighted graphs match full graph performance?\n",
    "\n",
    "**Comparative Analysis:**\n",
    "- **Jaccard vs Adamic-Adar vs Random**: Which metric better preserves important edges?\n",
    "- **Dataset Robustness**: Some datasets (e.g., Cora) may be more resilient to sparsification than others (e.g., Flickr)\n",
    "- **Sweet Spot**: Look for the retention ratio where Scenario D provides best accuracy/efficiency trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a5cba",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics per dataset\n",
    "summary = df.groupby([\"Dataset\", \"Metric\", \"Scenario\"]).agg({\n",
    "    \"Accuracy\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "    \"Epochs\": \"mean\",\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = [\"Mean Acc\", \"Std Acc\", \"Min Acc\", \"Max Acc\", \"Avg Epochs\"]\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa4a25a",
   "metadata": {},
   "source": [
    "## 7. Best Configuration per Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd512dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configuration per scenario and dataset\n",
    "best_configs = df.loc[df.groupby([\"Dataset\", \"Metric\", \"Scenario\"])[\"Accuracy\"].idxmax()]\n",
    "best_configs[[\"Dataset\", \"Metric\", \"Scenario\", \"Retention\", \"Accuracy\", \"Epochs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1866db5",
   "metadata": {},
   "source": [
    "## 8. Training Dynamics: Validation Accuracy Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4898df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training curves for a single configuration to visualize training dynamics\n",
    "loader_cora = DatasetLoader(root=str(DATA_ROOT))\n",
    "data_cora, num_features, num_classes = loader_cora.get_dataset(\"cora\", DEVICE)\n",
    "study = AblationStudy(data_cora, num_features, num_classes, DEVICE)\n",
    "\n",
    "training_curves = study.run_training_curves(\n",
    "    model_name=\"gcn\",\n",
    "    metric=\"jaccard\",\n",
    "    retention_ratio=0.6,\n",
    "    hidden_channels=64,\n",
    "    epochs=100,\n",
    "    patience=None,  # Train for full 100 epochs to see complete curves\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# Display the number of epochs for each scenario\n",
    "for scenario, val_acc_history in training_curves.items():\n",
    "    print(f\"{scenario}: {len(val_acc_history)} epochs\")\n",
    "\n",
    "# Clean up\n",
    "del loader_cora\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d587f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for all 4 scenarios\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "scenario_colors = {\n",
    "    \"A: Full + Binary\": \"#2ecc71\",\n",
    "    \"B: Sparse + Binary\": \"#3498db\",\n",
    "    \"C: Full + Weighted\": \"#e74c3c\",\n",
    "    \"D: Sparse + Weighted\": \"#9b59b6\",\n",
    "}\n",
    "\n",
    "for scenario, val_acc_history in training_curves.items():\n",
    "    epochs = range(1, len(val_acc_history) + 1)\n",
    "    plt.plot(\n",
    "        epochs,\n",
    "        np.array(val_acc_history) * 100,\n",
    "        color=scenario_colors[scenario],\n",
    "        linewidth=2,\n",
    "        label=scenario,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Validation Accuracy (%)\", fontsize=12)\n",
    "plt.title(\"Training Dynamics: Validation Accuracy Over Epochs\\nGCN on Cora (Jaccard, 60% retention)\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807607bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis: convergence speed and final performance\n",
    "convergence_stats = []\n",
    "\n",
    "for scenario, val_acc_history in training_curves.items():\n",
    "    val_acc_array = np.array(val_acc_history)\n",
    "    \n",
    "    # Find epoch when accuracy reaches 95% of maximum\n",
    "    max_acc = val_acc_array.max()\n",
    "    threshold = 0.95 * max_acc\n",
    "    convergence_epoch = np.argmax(val_acc_array >= threshold) + 1\n",
    "    \n",
    "    convergence_stats.append({\n",
    "        \"Scenario\": scenario,\n",
    "        \"Final Val Acc\": val_acc_array[-1],\n",
    "        \"Best Val Acc\": max_acc,\n",
    "        \"Convergence Epoch (95%)\": convergence_epoch,\n",
    "        \"Improvement (first 10 epochs)\": val_acc_array[9] - val_acc_array[0],\n",
    "    })\n",
    "\n",
    "convergence_df = pd.DataFrame(convergence_stats)\n",
    "convergence_df = convergence_df.round(4)\n",
    "convergence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541bb7da",
   "metadata": {},
   "source": [
    "## 9. Comparative Analysis: Three Metrics at a Glance\n",
    "\n",
    "With experiments now complete for all three sparsification metrics (Jaccard, Adamic-Adar, and Effective Resistance), let's compare their performance profiles and strategic differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0029056",
   "metadata": {},
   "source": [
    "## 9. Efficiency Analysis: Time & Memory\n",
    "\n",
    "We now analyze the computational cost of each method. This is critical for the \"Performance vs. Efficiency\" trade-off.\n",
    "\n",
    "**Metrics Tracked:**\n",
    "* **Preprocessing Time:** Time to compute edge weights and sparsify the graph.\n",
    "* **Training Time:** Wall-clock time for the full training run.\n",
    "* **Peak Memory:** Maximum GPU memory allocated during the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3107dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency Analysis Plots\n",
    "\n",
    "# Filter for relevant scenarios (Sparse + Weighted/Binary) to compare metrics\n",
    "# We focus on Scenario D (Sparse + Weighted) as it represents the full method\n",
    "efficiency_df = df[df[\"Scenario\"] == \"D: Sparse + Weighted\"].copy()\n",
    "\n",
    "unique_datasets = sorted(efficiency_df[\"Dataset\"].unique())\n",
    "fig, axes = plt.subplots(len(unique_datasets), 3, figsize=(18, 5 * len(unique_datasets)))\n",
    "axes = np.atleast_2d(axes)\n",
    "\n",
    "for row_idx, dataset in enumerate(unique_datasets):\n",
    "    ds_eff = efficiency_df[efficiency_df[\"Dataset\"] == dataset]\n",
    "    \n",
    "    # 1. Preprocessing Time vs Retention\n",
    "    ax = axes[row_idx, 0]\n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_data = ds_eff[ds_eff[\"Metric\"] == metric].sort_values(\"Retention\")\n",
    "        if len(metric_data) > 0:\n",
    "            ax.plot(metric_data[\"Retention\"], metric_data[\"PreprocessSec\"], \n",
    "                    marker=markers.get(metric, 'o'), label=metric, \n",
    "                    color=METRIC_COLOR.get(metric, 'gray'), linewidth=2, markersize=7)\n",
    "    ax.set_title(f\"{dataset}: Preprocessing Time\")\n",
    "    ax.set_xlabel(\"Retention Ratio\")\n",
    "    ax.set_ylabel(\"Time (seconds)\")\n",
    "    ax.invert_xaxis()\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Training Time vs Retention\n",
    "    ax = axes[row_idx, 1]\n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_data = ds_eff[ds_eff[\"Metric\"] == metric].sort_values(\"Retention\")\n",
    "        if len(metric_data) > 0:\n",
    "            ax.plot(metric_data[\"Retention\"], metric_data[\"TrainSec\"], \n",
    "                    marker=markers.get(metric, 'o'), label=metric, \n",
    "                    color=METRIC_COLOR.get(metric, 'gray'), linewidth=2, markersize=7)\n",
    "    ax.set_title(f\"{dataset}: Training Time\")\n",
    "    ax.set_xlabel(\"Retention Ratio\")\n",
    "    ax.set_ylabel(\"Time (seconds)\")\n",
    "    ax.invert_xaxis()\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Peak Memory vs Retention\n",
    "    ax = axes[row_idx, 2]\n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_data = ds_eff[ds_eff[\"Metric\"] == metric].sort_values(\"Retention\")\n",
    "        if len(metric_data) > 0:\n",
    "            ax.plot(metric_data[\"Retention\"], metric_data[\"PeakMemMB\"], \n",
    "                    marker=markers.get(metric, 'o'), label=metric, \n",
    "                    color=METRIC_COLOR.get(metric, 'gray'), linewidth=2, markersize=7)\n",
    "    ax.set_title(f\"{dataset}: Peak Memory Usage\")\n",
    "    ax.set_xlabel(\"Retention Ratio\")\n",
    "    ax.set_ylabel(\"Memory (MB)\")\n",
    "    ax.invert_xaxis()\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Efficiency Metrics: Time & Memory Scalability\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5a870",
   "metadata": {},
   "source": [
    "## 10. Pareto Frontier: Accuracy vs. Efficiency Trade-off\n",
    "\n",
    "This is the most critical visualization for proving the value of sparsification. We plot **Test Accuracy (Y-axis)** against **Total Time (X-axis)**.\n",
    "\n",
    "* **Goal:** Points in the **top-left** corner are ideal (High Accuracy, Low Time).\n",
    "* **Pareto Optimality:** A method is Pareto optimal if no other method is both faster and more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto Frontier Plot: Accuracy vs Training Time (The \"Money Plot\")\n",
    "# This is THE most important visualization for proving efficiency gains\n",
    "\n",
    "efficiency_df[\"TotalTime\"] = efficiency_df[\"PreprocessSec\"] + efficiency_df[\"TrainSec\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(unique_datasets), figsize=(7 * len(unique_datasets), 6))\n",
    "if len(unique_datasets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, dataset in enumerate(unique_datasets):\n",
    "    ax = axes[i]\n",
    "    ds_eff = efficiency_df[efficiency_df[\"Dataset\"] == dataset]\n",
    "    \n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_data = ds_eff[ds_eff[\"Metric\"] == metric].sort_values(\"Retention\")\n",
    "        \n",
    "        if len(metric_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Plot line connecting retention points\n",
    "        ax.plot(metric_data[\"TrainSec\"], metric_data[\"Accuracy\"], \n",
    "                linestyle='-', alpha=0.4, color=METRIC_COLOR.get(metric, 'gray'), linewidth=1.5)\n",
    "        \n",
    "        # Scatter points sized by retention ratio\n",
    "        scatter = ax.scatter(metric_data[\"TrainSec\"], metric_data[\"Accuracy\"], \n",
    "                             s=metric_data[\"Retention\"] * 200 + 50, \n",
    "                             c=METRIC_COLOR.get(metric, 'gray'), \n",
    "                             marker=markers.get(metric, 'o'),\n",
    "                             label=metric, alpha=0.85, edgecolors='white', linewidths=1)\n",
    "        \n",
    "        # Annotate key retention points\n",
    "        for _, row in metric_data.iterrows():\n",
    "            if row[\"Retention\"] in [0.3, 0.5, 0.7, 0.9]:\n",
    "                ax.annotate(f\"{row['Retention']:.0%}\", \n",
    "                            (row[\"TrainSec\"], row[\"Accuracy\"]),\n",
    "                            xytext=(5, 5), textcoords='offset points', fontsize=7, alpha=0.7)\n",
    "\n",
    "    # Add Baseline (Full Graph) reference point\n",
    "    baseline_data = df[(df[\"Dataset\"] == dataset) & (df[\"Scenario\"] == \"A: Full + Binary\")]\n",
    "    if len(baseline_data) > 0:\n",
    "        baseline_acc = baseline_data[\"Accuracy\"].mean()\n",
    "        baseline_time = baseline_data[\"TrainSec\"].mean()\n",
    "        ax.scatter(baseline_time, baseline_acc, marker='*', s=400, color='black', \n",
    "                   label='Baseline (Full)', zorder=10, edgecolors='gold', linewidths=2)\n",
    "    \n",
    "    ax.set_title(f\"Pareto Frontier: {dataset}\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Training Time (seconds)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='lower right', fontsize=9)\n",
    "    \n",
    "    # Add annotation for ideal region\n",
    "    ax.annotate(\"← Better (Fast & Accurate)\", xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                fontsize=9, style='italic', alpha=0.6)\n",
    "\n",
    "plt.suptitle(\"Accuracy vs. Training Time Trade-off\\\\n(Larger points = Higher Retention)\", \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583efe2b",
   "metadata": {},
   "source": [
    "### 9.2 Peak Memory vs. Retention Ratio\n",
    "\n",
    "This plot shows the memory savings achieved by sparsification. Lower retention = fewer edges = less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd45a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line Plot: Peak Memory vs Retention Ratio (Dedicated Plot)\n",
    "fig, axes = plt.subplots(1, len(unique_datasets), figsize=(6 * len(unique_datasets), 5))\n",
    "if len(unique_datasets) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, dataset in enumerate(unique_datasets):\n",
    "    ax = axes[i]\n",
    "    ds_eff = efficiency_df[efficiency_df[\"Dataset\"] == dataset]\n",
    "    \n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_data = ds_eff[ds_eff[\"Metric\"] == metric].sort_values(\"Retention\", ascending=False)\n",
    "        if len(metric_data) > 0:\n",
    "            ax.plot(metric_data[\"Retention\"], metric_data[\"PeakMemMB\"], \n",
    "                    marker=markers.get(metric, 'o'), \n",
    "                    label=metric, \n",
    "                    color=METRIC_COLOR.get(metric, 'gray'),\n",
    "                    linewidth=2.5, markersize=8, alpha=0.9)\n",
    "    \n",
    "    ax.set_title(f\"{dataset}: Memory Savings\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Retention Ratio\", fontsize=12)\n",
    "    ax.set_ylabel(\"Peak GPU Memory (MB)\", fontsize=12)\n",
    "    ax.invert_xaxis()  # Higher retention on left\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage reduction annotation at 50% retention\n",
    "    if len(ds_eff) > 0:\n",
    "        full_mem = ds_eff[ds_eff[\"Retention\"] >= 0.95][\"PeakMemMB\"].mean()\n",
    "        half_mem = ds_eff[ds_eff[\"Retention\"].between(0.45, 0.55)][\"PeakMemMB\"].mean()\n",
    "        if not np.isnan(full_mem) and not np.isnan(half_mem) and full_mem > 0:\n",
    "            reduction = (1 - half_mem / full_mem) * 100\n",
    "            ax.annotate(f\"~{reduction:.0f}% reduction\\\\nat 50% retention\", \n",
    "                        xy=(0.5, half_mem), xycoords=('data', 'data'),\n",
    "                        xytext=(0.6, half_mem * 1.1), textcoords=('data', 'data'),\n",
    "                        fontsize=9, alpha=0.7,\n",
    "                        arrowprops=dict(arrowstyle='->', alpha=0.5))\n",
    "\n",
    "plt.suptitle(\"Peak Memory Usage vs. Edge Retention\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331566c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparative analysis per dataset\n",
    "comparison_rows = []\n",
    "for dataset in sorted(df[\"Dataset\"].unique()):\n",
    "    df_ds = df[df[\"Dataset\"] == dataset]\n",
    "    for metric in METRIC_LABELS:\n",
    "        metric_df = df_ds[df_ds[\"Metric\"] == metric]\n",
    "        baseline = metric_df[(metric_df[\"Retention\"] == 1.0) & (metric_df[\"Scenario\"] == \"A: Full + Binary\")]\n",
    "        if len(baseline) > 0:\n",
    "            best_baseline = baseline[\"Accuracy\"].values[0]\n",
    "        else:\n",
    "            baseline_90 = metric_df[(metric_df[\"Retention\"] == 0.9) & (metric_df[\"Scenario\"] == \"A: Full + Binary\")]\n",
    "            best_baseline = baseline_90[\"Accuracy\"].values[0] if len(baseline_90) > 0 else 0\n",
    "        best_overall = metric_df[\"Accuracy\"].max()\n",
    "        best_at = metric_df[metric_df[\"Accuracy\"] == best_overall].iloc[0]\n",
    "        worst_overall = metric_df[\"Accuracy\"].min()\n",
    "        avg_acc = metric_df[\"Accuracy\"].mean()\n",
    "        comparison_rows.append({\n",
    "            \"Dataset\": dataset,\n",
    "            \"Metric\": metric,\n",
    "            \"Baseline Acc\": best_baseline,\n",
    "            \"Best Acc\": best_overall,\n",
    "            \"Best Config\": f\"{best_at['Scenario']} @ {best_at['Retention']:.0%}\",\n",
    "            \"Worst Acc\": worst_overall,\n",
    "            \"Avg Acc\": avg_acc,\n",
    "            \"Range\": best_overall - worst_overall,\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows).round(4)\n",
    "print(\"\\nMetric Performance Comparison (per dataset):\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize per dataset\n",
    "for dataset in sorted(df[\"Dataset\"].unique()):\n",
    "    df_cmp = comparison_df[comparison_df[\"Dataset\"] == dataset]\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    metrics = METRIC_LABELS\n",
    "    x_pos = np.arange(len(metrics))\n",
    "    width = 0.2\n",
    "    baseline_accs = [df_cmp[df_cmp[\"Metric\"] == m][\"Baseline Acc\"].values[0] for m in metrics]\n",
    "    best_accs = [df_cmp[df_cmp[\"Metric\"] == m][\"Best Acc\"].values[0] for m in metrics]\n",
    "    avg_accs = [df_cmp[df_cmp[\"Metric\"] == m][\"Avg Acc\"].values[0] for m in metrics]\n",
    "    ax.bar(x_pos - width, baseline_accs, width, label=\"Baseline (100% Full/Binary)\", color=\"#2ecc71\", alpha=0.8)\n",
    "    ax.bar(x_pos, best_accs, width, label=\"Best Configuration\", color=\"#e74c3c\", alpha=0.8)\n",
    "    ax.bar(x_pos + width, avg_accs, width, label=\"Average Across All Configs\", color=\"#9b59b6\", alpha=0.8)\n",
    "    ax.set_xlabel(\"Sparsification Metric\", fontsize=12)\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=12)\n",
    "    ax.set_title(f\"Performance Comparison - {dataset}\", fontsize=14)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    for i, (baseline, best, avg) in enumerate(zip(baseline_accs, best_accs, avg_accs)):\n",
    "        ax.text(i - width, baseline + 0.005, f\"{baseline:.3f}\", ha=\"center\", fontsize=9)\n",
    "        ax.text(i, best + 0.005, f\"{best:.3f}\", ha=\"center\", fontsize=9)\n",
    "        ax.text(i + width, avg + 0.005, f\"{avg:.3f}\", ha=\"center\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f687a2e7",
   "metadata": {},
   "source": [
    "## 10. Graph Topology After Sparsification\n",
    "\n",
    "Now we analyze how sparsification affects the graph topology. We compare the original graph with sparsified versions at different retention ratios to understand structural changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora for topology analysis (small dataset, fast to compute)\n",
    "loader_topo = DatasetLoader(root=str(DATA_ROOT))\n",
    "data, _, _ = loader_topo.get_dataset(\"cora\", DEVICE)\n",
    "\n",
    "# Initialize sparsifier\n",
    "sparsifier = GraphSparsifier(data, DEVICE)\n",
    "\n",
    "# Compute stats for original and sparsified graphs\n",
    "topology_data = []\n",
    "\n",
    "# Original graph\n",
    "original_stats = compute_graph_stats(data)\n",
    "topology_data.append({\n",
    "    \"Retention\": \"100%\",\n",
    "    \"Metric\": \"Original\",\n",
    "    **original_stats\n",
    "})\n",
    "\n",
    "# Sparsified graphs with all metrics (including random baseline)\n",
    "retention_ratios = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3]\n",
    "for ratio in retention_ratios:\n",
    "    for metric in METRIC_KEYS:\n",
    "        sparse_data = sparsifier.sparsify(metric, ratio)\n",
    "        stats = compute_graph_stats(sparse_data)\n",
    "        # Normalize metric name to match METRIC_LABELS\n",
    "        metric_label = metric.replace(\"_\", \" \").title().replace(\"Approx Er\", \"Approx ER\")\n",
    "        topology_data.append({\n",
    "            \"Retention\": f\"{ratio:.0%}\",\n",
    "            \"Metric\": metric_label,\n",
    "            **stats\n",
    "        })\n",
    "\n",
    "topology_df = pd.DataFrame(topology_data)\n",
    "\n",
    "# Clean up to free memory\n",
    "del loader_topo, sparsifier\n",
    "gc.collect()\n",
    "\n",
    "topology_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af21b83d",
   "metadata": {},
   "source": [
    "### 9.1 Topology Changes Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad268937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert retention ratio strings to numeric values for plotting\n",
    "topology_df[\"Retention_numeric\"] = topology_df[\"Retention\"].apply(retention_to_numeric)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "linestyles = {\"Jaccard\": \"-\", \"Adamic Adar\": \"--\", \"Approx ER\": \":\", \"Random\": \"-.\"}\n",
    "markers = {\"Jaccard\": \"o\", \"Adamic Adar\": \"s\", \"Approx ER\": \"D\", \"Random\": \"x\"}\n",
    "\n",
    "x_ticks = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Plot 1: Average Degree\n",
    "for metric in METRIC_LABELS:\n",
    "    metric_df = topology_df[topology_df[\"Metric\"] == metric].sort_values(\"Retention_numeric\")\n",
    "    if len(metric_df) == 0:\n",
    "        continue\n",
    "    axes[0, 0].plot(\n",
    "        metric_df[\"Retention_numeric\"],\n",
    "        metric_df[\"avg_degree\"],\n",
    "        marker=markers.get(metric, \"o\"),\n",
    "        label=metric,\n",
    "        linewidth=2.5,\n",
    "        markersize=7,\n",
    "        color=METRIC_COLOR.get(metric, \"#7f8c8d\"),\n",
    "        linestyle=linestyles.get(metric, \"-\"),\n",
    "        zorder=3,\n",
    "        alpha=0.95,\n",
    "    )\n",
    "axes[0, 0].axhline(\n",
    "    topology_df[topology_df[\"Metric\"] == \"Original\"][\"avg_degree\"].values[0],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Original\",\n",
    "    alpha=0.7,\n",
    "    zorder=2,\n",
    " )\n",
    "axes[0, 0].set_xlabel(\"Retention Ratio\")\n",
    "axes[0, 0].set_ylabel(\"Average Degree\")\n",
    "axes[0, 0].set_title(\"Average Degree vs Retention\")\n",
    "axes[0, 0].set_xticks(x_ticks)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Degree Standard Deviation\n",
    "for metric in METRIC_LABELS:\n",
    "    metric_df = topology_df[topology_df[\"Metric\"] == metric].sort_values(\"Retention_numeric\")\n",
    "    if len(metric_df) == 0:\n",
    "        continue\n",
    "    axes[0, 1].plot(\n",
    "        metric_df[\"Retention_numeric\"],\n",
    "        metric_df[\"std_degree\"],\n",
    "        marker=markers.get(metric, \"o\"),\n",
    "        label=metric,\n",
    "        linewidth=2.5,\n",
    "        markersize=7,\n",
    "        color=METRIC_COLOR.get(metric, \"#7f8c8d\"),\n",
    "        linestyle=linestyles.get(metric, \"-\"),\n",
    "        zorder=3,\n",
    "        alpha=0.95,\n",
    "    )\n",
    "axes[0, 1].axhline(\n",
    "    topology_df[topology_df[\"Metric\"] == \"Original\"][\"std_degree\"].values[0],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Original\",\n",
    "    alpha=0.7,\n",
    "    zorder=2,\n",
    " )\n",
    "axes[0, 1].set_xlabel(\"Retention Ratio\")\n",
    "axes[0, 1].set_ylabel(\"Degree Std Dev\")\n",
    "axes[0, 1].set_title(\"Degree Standard Deviation vs Retention\")\n",
    "axes[0, 1].set_xticks(x_ticks)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Graph Density\n",
    "for metric in METRIC_LABELS:\n",
    "    metric_df = topology_df[topology_df[\"Metric\"] == metric].sort_values(\"Retention_numeric\")\n",
    "    if len(metric_df) == 0:\n",
    "        continue\n",
    "    axes[1, 0].plot(\n",
    "        metric_df[\"Retention_numeric\"],\n",
    "        metric_df[\"density\"],\n",
    "        marker=markers.get(metric, \"o\"),\n",
    "        label=metric,\n",
    "        linewidth=2.5,\n",
    "        markersize=7,\n",
    "        color=METRIC_COLOR.get(metric, \"#7f8c8d\"),\n",
    "        linestyle=linestyles.get(metric, \"-\"),\n",
    "        zorder=3,\n",
    "        alpha=0.95,\n",
    "    )\n",
    "axes[1, 0].axhline(\n",
    "    topology_df[topology_df[\"Metric\"] == \"Original\"][\"density\"].values[0],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Original\",\n",
    "    alpha=0.7,\n",
    "    zorder=2,\n",
    " )\n",
    "axes[1, 0].set_xlabel(\"Retention Ratio\")\n",
    "axes[1, 0].set_ylabel(\"Graph Density\")\n",
    "axes[1, 0].set_title(\"Graph Density vs Retention\")\n",
    "axes[1, 0].set_xticks(x_ticks)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Maximum Degree\n",
    "for metric in METRIC_LABELS:\n",
    "    metric_df = topology_df[topology_df[\"Metric\"] == metric].sort_values(\"Retention_numeric\")\n",
    "    if len(metric_df) == 0:\n",
    "        continue\n",
    "    axes[1, 1].plot(\n",
    "        metric_df[\"Retention_numeric\"],\n",
    "        metric_df[\"max_degree\"],\n",
    "        marker=markers.get(metric, \"o\"),\n",
    "        label=metric,\n",
    "        linewidth=2.5,\n",
    "        markersize=7,\n",
    "        color=METRIC_COLOR.get(metric, \"#7f8c8d\"),\n",
    "        linestyle=linestyles.get(metric, \"-\"),\n",
    "        zorder=3,\n",
    "        alpha=0.95,\n",
    "    )\n",
    "axes[1, 1].axhline(\n",
    "    topology_df[topology_df[\"Metric\"] == \"Original\"][\"max_degree\"].values[0],\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Original\",\n",
    "    alpha=0.7,\n",
    "    zorder=2,\n",
    " )\n",
    "axes[1, 1].set_xlabel(\"Retention Ratio\")\n",
    "axes[1, 1].set_ylabel(\"Max Degree\")\n",
    "axes[1, 1].set_title(\"Maximum Degree vs Retention\")\n",
    "axes[1, 1].set_xticks(x_ticks)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Graph Topology Changes Across Sparsification Metrics\", fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69419c5",
   "metadata": {},
   "source": [
    "### Interpretation: Pareto Frontier\n",
    "- Trade-offs: Observe accuracy vs. edge count; highlight sparsity-efficiency gains.\n",
    "- Frontier shifts: Weighted sparse graphs should push frontier up-left.\n",
    "- Missing ER for Flickr: Frontier excludes those points to maintain comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc514e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check data availability for each metric\n",
    "print(\"Data availability check:\")\n",
    "for metric in METRIC_LABELS:\n",
    "    metric_data = topology_df[topology_df[\"Metric\"] == metric].sort_values(\"Retention_numeric\")\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Rows: {len(metric_data)}\")\n",
    "    print(f\"  Retention values (sorted): {metric_data['Retention_numeric'].values}\")\n",
    "    print(f\"  Avg degree values: {metric_data['avg_degree'].values}\")\n",
    "    print(f\"  Has NaN in density: {metric_data['density'].isna().any()}\")\n",
    "    print(metric_data[[\"Retention\", \"Retention_numeric\", \"avg_degree\", \"density\"]].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc9522",
   "metadata": {},
   "source": [
    "### 9.2 Key Observations\n",
    "\n",
    "**Topology Changes:**\n",
    "\n",
    "1. **Average Degree**: Both metrics produce proportional reductions in average degree as retention decreases, which is expected since we're removing edges systematically.\n",
    "\n",
    "2. **Degree Distribution**: The standard deviation of node degrees shows how sparsification affects graph uniformity. Lower std dev indicates more uniform connectivity across nodes.\n",
    "\n",
    "3. **Graph Density**: Tracks the fraction of possible edges that remain. This decreases roughly linearly with retention ratio.\n",
    "\n",
    "4. **Max Degree**: Shows whether sparsification preferentially removes edges from high-degree nodes (hubs) or distributes removals uniformly.\n",
    "\n",
    "**Metric Comparison:**\n",
    "- **Jaccard** vs **Adamic-Adar** may produce similar edge counts but different structural properties\n",
    "- Differences in max degree and std dev reveal how each metric prioritizes edges around hubs vs peripheral nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bf9f5",
   "metadata": {},
   "source": [
    "### 9.3 Before/After Comparison Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wubmo9sp1zj",
   "metadata": {},
   "source": [
    "## 11. Geodesic Preservation Analysis\n",
    "\n",
    "Measure how well each sparsification method preserves shortest path distances. The **metric backbone** guarantees 100% geodesic preservation by construction, while threshold-based methods may distort distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3vvwfjc4dev",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute geodesic preservation for different sparsification configurations\n",
    "from src.sparsification import compute_geodesic_preservation, compute_topology_preservation\n",
    "from src.sparsification.core import GraphSparsifier\n",
    "from src.data import DatasetLoader, SAFE_DATASETS\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Load datasets - use ALL safe datasets (excluding polblogs which has 0 node features)\n",
    "loader = DatasetLoader(root=\"../data\")\n",
    "DATASETS_NO_FEATURES = [\"polblogs\"]\n",
    "ANALYSIS_DATASETS = [d for d in SAFE_DATASETS if d not in DATASETS_NO_FEATURES]\n",
    "\n",
    "# Large datasets - use fewer samples for geodesic computation\n",
    "LARGE_DATASETS = [\"flickr\", \"cs\", \"physics\", \"corafull\", \"ppi\"]\n",
    "\n",
    "geodesic_results = []\n",
    "topology_results = []\n",
    "metrics_to_test = [\"jaccard\", \"adamic_adar\", \"approx_er\"]\n",
    "retention_ratios = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "print(f\"Computing geodesic and topology preservation for {len(ANALYSIS_DATASETS)} datasets...\")\n",
    "print(f\"Datasets: {ANALYSIS_DATASETS}\")\n",
    "\n",
    "for dataset_name in ANALYSIS_DATASETS:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Dataset: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    data, _, _ = loader.get_dataset(dataset_name, \"cpu\")\n",
    "    n = data.num_nodes\n",
    "    edge_index = data.edge_index.cpu().numpy()\n",
    "    adj = sp.csr_matrix(\n",
    "        (np.ones(edge_index.shape[1]), (edge_index[0], edge_index[1])),\n",
    "        shape=(n, n)\n",
    "    )\n",
    "    \n",
    "    # Use fewer samples for large datasets\n",
    "    n_samples = 100 if dataset_name in LARGE_DATASETS else 200\n",
    "    \n",
    "    sparsifier = GraphSparsifier(data, \"cpu\")\n",
    "    \n",
    "    for metric in metrics_to_test:\n",
    "        for retention in retention_ratios:\n",
    "            # Threshold-based sparsification\n",
    "            sparse_data = sparsifier.sparsify(metric, retention)\n",
    "            sparse_edge_index = sparse_data.edge_index.cpu().numpy()\n",
    "            sparse_adj = sp.csr_matrix(\n",
    "                (np.ones(sparse_edge_index.shape[1]), (sparse_edge_index[0], sparse_edge_index[1])),\n",
    "                shape=(n, n)\n",
    "            )\n",
    "            \n",
    "            # Compute geodesic preservation\n",
    "            geo_result = compute_geodesic_preservation(adj, sparse_adj, n_samples=n_samples)\n",
    "            geodesic_results.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Method\": metric.replace(\"_\", \" \").title(),\n",
    "                \"Retention\": retention,\n",
    "                \"Type\": \"Threshold\",\n",
    "                \"Preservation\": geo_result[\"preservation_ratio\"],\n",
    "                \"Disconnected\": geo_result[\"disconnected_count\"],\n",
    "            })\n",
    "            \n",
    "            # Compute topology preservation\n",
    "            topo_result = compute_topology_preservation(adj, sparse_adj)\n",
    "            topology_results.append({\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Method\": metric.replace(\"_\", \" \").title(),\n",
    "                \"Retention\": retention,\n",
    "                \"Type\": \"Threshold\",\n",
    "                \"ClusteringPreservation\": topo_result[\"clustering_preservation\"],\n",
    "                \"ConnectivityPreservation\": topo_result[\"connectivity_preservation\"],\n",
    "            })\n",
    "        \n",
    "        # Add metric backbone\n",
    "        sparse_data, stats = sparsifier.sparsify_metric_backbone(metric=metric)\n",
    "        sparse_edge_index = sparse_data.edge_index.cpu().numpy()\n",
    "        sparse_adj = sp.csr_matrix(\n",
    "            (np.ones(sparse_edge_index.shape[1]), (sparse_edge_index[0], sparse_edge_index[1])),\n",
    "            shape=(n, n)\n",
    "        )\n",
    "        \n",
    "        geo_result = compute_geodesic_preservation(adj, sparse_adj, n_samples=n_samples)\n",
    "        topo_result = compute_topology_preservation(adj, sparse_adj)\n",
    "        \n",
    "        geodesic_results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Method\": metric.replace(\"_\", \" \").title(),\n",
    "            \"Retention\": stats[\"retention_ratio\"],\n",
    "            \"Type\": \"MetricBackbone\",\n",
    "            \"Preservation\": geo_result[\"preservation_ratio\"],\n",
    "            \"Disconnected\": geo_result[\"disconnected_count\"],\n",
    "        })\n",
    "        \n",
    "        topology_results.append({\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Method\": metric.replace(\"_\", \" \").title(),\n",
    "            \"Retention\": stats[\"retention_ratio\"],\n",
    "            \"Type\": \"MetricBackbone\",\n",
    "            \"ClusteringPreservation\": topo_result[\"clustering_preservation\"],\n",
    "            \"ConnectivityPreservation\": topo_result[\"connectivity_preservation\"],\n",
    "        })\n",
    "    \n",
    "    print(f\"  Completed {len(metrics_to_test) * (len(retention_ratios) + 1)} configurations\")\n",
    "\n",
    "geodesic_df = pd.DataFrame(geodesic_results)\n",
    "topology_df = pd.DataFrame(topology_results)\n",
    "print(f\"\\nGeodesic preservation computed for {len(geodesic_df)} configurations across {len(ANALYSIS_DATASETS)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cp4fgmhqg2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize geodesic and topology preservation - Summary across all datasets\n",
    "colors = {\"Jaccard\": \"#2ecc71\", \"Approx Er\": \"#e74c3c\", \"Adamic Adar\": \"#3498db\"}\n",
    "\n",
    "# Figure 1: Summary across ALL datasets (averaged)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Average geodesic preservation by method and retention\n",
    "threshold_geo = geodesic_df[geodesic_df[\"Type\"] == \"Threshold\"]\n",
    "backbone_geo = geodesic_df[geodesic_df[\"Type\"] == \"MetricBackbone\"]\n",
    "\n",
    "ax1 = axes[0]\n",
    "for method in threshold_geo[\"Method\"].unique():\n",
    "    method_data = threshold_geo[threshold_geo[\"Method\"] == method]\n",
    "    avg_by_retention = method_data.groupby(\"Retention\")[\"Preservation\"].mean()\n",
    "    std_by_retention = method_data.groupby(\"Retention\")[\"Preservation\"].std()\n",
    "    ax1.errorbar(avg_by_retention.index, avg_by_retention.values, \n",
    "                 yerr=std_by_retention.values, marker=\"o\", label=f\"{method}\", \n",
    "                 color=colors.get(method, \"gray\"), capsize=3)\n",
    "\n",
    "# Backbone reference\n",
    "ax1.axhline(1.0, color=\"black\", linestyle=\"--\", alpha=0.5, label=\"Backbone (100%)\")\n",
    "backbone_avg = backbone_geo.groupby(\"Method\")[[\"Retention\", \"Preservation\"]].mean()\n",
    "for method, row in backbone_avg.iterrows():\n",
    "    ax1.scatter(row[\"Retention\"], row[\"Preservation\"], \n",
    "                marker=\"*\", s=200, color=colors.get(method, \"gray\"),\n",
    "                edgecolors=\"black\", linewidths=1.5, zorder=5)\n",
    "\n",
    "ax1.set_xlabel(\"Retention Ratio\")\n",
    "ax1.set_ylabel(\"Geodesic Preservation (Mean ± Std)\")\n",
    "ax1.set_title(f\"Geodesic Preservation (Averaged over {len(ANALYSIS_DATASETS)} datasets)\")\n",
    "ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "ax1.set_ylim(0.5, 1.05)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Average clustering preservation by method and retention\n",
    "threshold_topo = topology_df[topology_df[\"Type\"] == \"Threshold\"]\n",
    "backbone_topo = topology_df[topology_df[\"Type\"] == \"MetricBackbone\"]\n",
    "\n",
    "ax2 = axes[1]\n",
    "for method in threshold_topo[\"Method\"].unique():\n",
    "    method_data = threshold_topo[threshold_topo[\"Method\"] == method]\n",
    "    avg_by_retention = method_data.groupby(\"Retention\")[\"ClusteringPreservation\"].mean()\n",
    "    std_by_retention = method_data.groupby(\"Retention\")[\"ClusteringPreservation\"].std()\n",
    "    ax2.errorbar(avg_by_retention.index, avg_by_retention.values, \n",
    "                 yerr=std_by_retention.values, marker=\"o\", label=f\"{method}\", \n",
    "                 color=colors.get(method, \"gray\"), capsize=3)\n",
    "\n",
    "backbone_avg = backbone_topo.groupby(\"Method\")[[\"Retention\", \"ClusteringPreservation\"]].mean()\n",
    "for method, row in backbone_avg.iterrows():\n",
    "    ax2.scatter(row[\"Retention\"], row[\"ClusteringPreservation\"], \n",
    "                marker=\"*\", s=200, color=colors.get(method, \"gray\"),\n",
    "                edgecolors=\"black\", linewidths=1.5, zorder=5)\n",
    "\n",
    "ax2.axhline(1.0, color=\"gray\", linestyle=\":\", alpha=0.5)\n",
    "ax2.set_xlabel(\"Retention Ratio\")\n",
    "ax2.set_ylabel(\"Clustering Preservation (Mean ± Std)\")\n",
    "ax2.set_title(f\"Clustering Preservation (Averaged over {len(ANALYSIS_DATASETS)} datasets)\")\n",
    "ax2.legend(loc=\"lower right\", fontsize=9)\n",
    "ax2.set_ylim(0, 1.5)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Structural Preservation: Threshold vs Metric Backbone (Stars)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Figure 2: Heatmap of geodesic preservation by dataset\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "pivot_df = threshold_geo.pivot_table(\n",
    "    index=\"Dataset\", \n",
    "    columns=[\"Method\", \"Retention\"], \n",
    "    values=\"Preservation\"\n",
    ").round(3)\n",
    "sns.heatmap(pivot_df, annot=True, fmt=\".2f\", cmap=\"RdYlGn\", ax=ax, vmin=0.6, vmax=1.0, annot_kws={\"size\": 8})\n",
    "ax.set_title(\"Geodesic Preservation by Dataset, Method, and Retention\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GEODESIC PRESERVATION SUMMARY (All Datasets)\")\n",
    "print(\"=\" * 80)\n",
    "summary = geodesic_df.groupby([\"Type\", \"Method\"]).agg({\n",
    "    \"Preservation\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "    \"Retention\": \"mean\",\n",
    "}).round(3)\n",
    "print(summary.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLUSTERING PRESERVATION SUMMARY (All Datasets)\")\n",
    "print(\"=\" * 80)\n",
    "summary_topo = topology_df.groupby([\"Type\", \"Method\"]).agg({\n",
    "    \"ClusteringPreservation\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "    \"ConnectivityPreservation\": [\"mean\", \"min\"],\n",
    "    \"Retention\": \"mean\",\n",
    "}).round(3)\n",
    "print(summary_topo.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7832da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create before/after comparison for key retention ratios\n",
    "comparison_data = []\n",
    "\n",
    "original = topology_df[topology_df[\"Metric\"] == \"Original\"].iloc[0]\n",
    "\n",
    "for ratio in [0.9, 0.7, 0.5, 0.3]:\n",
    "    for metric in METRIC_LABELS:\n",
    "        sparse = topology_df[\n",
    "            (topology_df[\"Retention\"] == f\"{ratio:.0%}\") & \n",
    "            (topology_df[\"Metric\"] == metric)\n",
    "        ].iloc[0]\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"Retention\": f\"{ratio:.0%}\",\n",
    "            \"Metric\": metric,\n",
    "            \"Edges Before\": original[\"num_edges\"],\n",
    "            \"Edges After\": sparse[\"num_edges\"],\n",
    "            \"Edges Removed\": original[\"num_edges\"] - sparse[\"num_edges\"],\n",
    "            \"% Removed\": f\"{(1-ratio)*100:.0f}%\",\n",
    "            \"Avg Degree Before\": f\"{original['avg_degree']:.2f}\",\n",
    "            \"Avg Degree After\": f\"{sparse['avg_degree']:.2f}\",\n",
    "            \"Degree Change\": f\"{sparse['avg_degree'] - original['avg_degree']:.2f}\",\n",
    "            \"Max Degree Before\": original[\"max_degree\"],\n",
    "            \"Max Degree After\": sparse[\"max_degree\"],\n",
    "            \"Max Degree Change\": sparse[\"max_degree\"] - original[\"max_degree\"],\n",
    "            \"Density Before\": f\"{original['density']:.6f}\",\n",
    "            \"Density After\": f\"{sparse['density']:.6f}\",\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681639e",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aec6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results per dataset and save figures\n",
    "output_dir = RESULTS_ROOT\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save combined CSVs (effect_df already saved earlier, just save summary)\n",
    "df.to_csv(output_dir / \"ablation_results_all_datasets.csv\", index=False)\n",
    "summary.to_csv(output_dir / \"summary_statistics_all_datasets.csv\")\n",
    "\n",
    "# Save per-dataset subsets\n",
    "for dataset in sorted(df[\"Dataset\"].unique()):\n",
    "    ds_dir = output_dir / dataset\n",
    "    figs_dir = ds_dir / \"figures\"\n",
    "    ds_dir.mkdir(exist_ok=True)\n",
    "    figs_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df[df[\"Dataset\"] == dataset].to_csv(ds_dir / f\"ablation_results_{dataset}.csv\", index=False)\n",
    "    summary.loc[dataset].to_csv(ds_dir / f\"summary_statistics_{dataset}.csv\")\n",
    "\n",
    "print(f\"✓ Results exported to {output_dir.resolve()} and per-dataset subfolders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06298eea",
   "metadata": {},
   "source": [
    "## 13. Summary\n",
    "\n",
    "This notebook provided comprehensive analysis comparing **threshold-based** vs **metric backbone** sparsification:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Accuracy**: Both methods can achieve comparable accuracy at similar retention ratios\n",
    "2. **Geodesic Preservation**: Metric backbone guarantees 100% shortest path preservation; threshold methods show degradation at low retention\n",
    "3. **Clustering Preservation**: Jaccard and Adamic-Adar better preserve clustering coefficient\n",
    "4. **Efficiency**: Threshold-based methods are faster to compute; metric backbone requires APSP\n",
    "\n",
    "### Method Selection Guide:\n",
    "\n",
    "| Requirement | Recommended Method |\n",
    "|-------------|-------------------|\n",
    "| Exact shortest paths | Metric Backbone |\n",
    "| Maximum compression | Threshold (Approx ER) |\n",
    "| Preserve local structure | Threshold (Jaccard/AA) |\n",
    "| Fast computation | Threshold (any metric) |\n",
    "| Configurable retention | Threshold (any metric) |\n",
    "\n",
    "### Files Exported:\n",
    "- `ablation_results_comprehensive.csv`: Full experiment results\n",
    "- Visualizations saved in notebook outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
