{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8b7e76",
   "metadata": {},
   "source": [
    "# Ablation Study: Sparsification vs. Weighting\n",
    "\n",
    "## Introduction\n",
    "This notebook conducts a rigorous ablation study to disentangle the effects of graph structure modification (sparsification) from edge weighting. We compare **two sparsification approaches**:\n",
    "\n",
    "1. **Threshold-based sparsification**: Remove edges below a score threshold (configurable retention ratio)\n",
    "2. **Metric backbone sparsification**: Remove semi-metric edges that violate triangle inequality (fixed retention ratio)\n",
    "\n",
    "**Objectives:**\n",
    "1. Compare four distinct scenarios for each sparsification method:\n",
    "    *   **A:** Baseline (Full Graph + Binary Weights)\n",
    "    *   **B:** Structure Only (Sparse Graph + Binary Weights)\n",
    "    *   **C:** Weighting Only (Full Graph + Weighted Edges)\n",
    "    *   **D:** Combined (Sparse Graph + Weighted Edges)\n",
    "2. Compare different edge metrics: Jaccard, Adamic-Adar, Approximate Effective Resistance\n",
    "3. Compare different GNN architectures: GCN, GAT, GraphSAGE\n",
    "4. Analyze with multiple seeds for statistical significance\n",
    "5. Measure: accuracy, training time, sparsification time, memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c177d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb308a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = \"mps\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa78edc",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f25cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import DatasetLoader, SAFE_DATASETS\n",
    "\n",
    "loader = DatasetLoader(root=\"../data\")\n",
    "\n",
    "# Use ALL safe datasets (excluding polblogs which has 0 node features)\n",
    "DATASETS_NO_FEATURES = [\"polblogs\"]\n",
    "DATASET_NAMES = [d for d in SAFE_DATASETS if d not in DATASETS_NO_FEATURES]\n",
    "\n",
    "print(f\"Running experiments on {len(DATASET_NAMES)} datasets:\")\n",
    "print(f\"  {DATASET_NAMES}\")\n",
    "\n",
    "datasets = {}\n",
    "stats_summary = {}\n",
    "\n",
    "for name in DATASET_NAMES:\n",
    "    print(f\"Loading {name}...\", end=\" \")\n",
    "    data, num_features, num_classes = loader.get_dataset(name, DEVICE)\n",
    "    datasets[name] = {\n",
    "        \"data\": data,\n",
    "        \"num_features\": num_features,\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    stats_summary[name] = {\n",
    "        \"Nodes\": int(data.num_nodes),\n",
    "        \"Edges\": int(data.edge_index.shape[1]),\n",
    "        \"Features\": int(num_features),\n",
    "        \"Classes\": int(num_classes)\n",
    "    }\n",
    "    print(f\"({data.num_nodes:,} nodes, {data.edge_index.shape[1]:,} edges)\")\n",
    "\n",
    "print_text_table(stats_summary, title=\"Dataset Summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d9489a",
   "metadata": {},
   "source": [
    "## 2. Single Ablation Study: Threshold vs Metric Backbone\n",
    "\n",
    "First, let's run a single comparison between threshold-based (60% retention) and metric backbone sparsification using Jaccard similarity on the Cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33474985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single comparison on Cora (smallest dataset for quick demo)\n",
    "demo_dataset = \"cora\"\n",
    "demo_data = datasets[demo_dataset][\"data\"]\n",
    "demo_features = datasets[demo_dataset][\"num_features\"]\n",
    "demo_classes = datasets[demo_dataset][\"num_classes\"]\n",
    "\n",
    "study = AblationStudy(\n",
    "    data=demo_data,\n",
    "    num_features=demo_features,\n",
    "    num_classes=demo_classes,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Run threshold-based sparsification (60% retention)\n",
    "print(\"=\" * 60)\n",
    "print(f\"THRESHOLD-BASED SPARSIFICATION (60% retention) - {demo_dataset.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "threshold_results_df = study.run_full_study(\n",
    "    model_name=\"gcn\",\n",
    "    metric=\"jaccard\",\n",
    "    retention_ratio=0.6,\n",
    "    hidden_channels=64,\n",
    "    epochs=200,\n",
    "    patience=20,\n",
    "    use_metric_backbone=False,\n",
    ")\n",
    "threshold_results_df[\"SparsificationType\"] = \"Threshold (60%)\"\n",
    "threshold_results_df[\"Dataset\"] = demo_dataset\n",
    "\n",
    "# Run metric backbone sparsification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"METRIC BACKBONE SPARSIFICATION - {demo_dataset.upper()}\")\n",
    "print(\"=\" * 60)\n",
    "backbone_results_df = study.run_full_study(\n",
    "    model_name=\"gcn\",\n",
    "    metric=\"jaccard\",\n",
    "    retention_ratio=0.6,  # Ignored when use_metric_backbone=True\n",
    "    hidden_channels=64,\n",
    "    epochs=200,\n",
    "    patience=20,\n",
    "    use_metric_backbone=True,\n",
    ")\n",
    "backbone_results_df[\"SparsificationType\"] = \"Metric Backbone\"\n",
    "backbone_results_df[\"Dataset\"] = demo_dataset\n",
    "\n",
    "# Combine results\n",
    "single_comparison_df = pd.concat([threshold_results_df, backbone_results_df], ignore_index=True)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: Threshold vs Metric Backbone\")\n",
    "print(\"=\" * 60)\n",
    "print(single_comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bea517",
   "metadata": {},
   "source": [
    "### 2.1 Decomposition of Effects\n",
    "\n",
    "**(Retrospective)** The ablation experiment has completed, and we have obtained the raw accuracy scores for all four scenarios (A, B, C, D) displayed in the table above.\n",
    "\n",
    "**(Prospective)** We will now mathematically decompose these results to isolate specific contributions. Using the formulas defined below, we will calculate the exact percentage gain or loss attributed to **Structure** (metric backbone pruning) versus **Weighting** (edge importance).\n",
    "\n",
    "* **Structure Effect:** $Acc_B - Acc_A$ (Impact of metric backbone sparsification)\n",
    "* **Weighting Effect:** $Acc_C - Acc_A$ (Impact of re-weighting edges)\n",
    "* **Combined Effect:** $Acc_D - Acc_A$ (Impact of doing both)\n",
    "* **Interaction:** $Acc_D - Acc_B - Acc_C + Acc_A$ (Synergy between structure and weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e75675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are already printed in the previous cell\n",
    "# This cell shows the effect analysis for the single comparison\n",
    "\n",
    "# Extract accuracies for effect calculation (using threshold results)\n",
    "baseline = threshold_results_df[threshold_results_df[\"Scenario\"] == \"A: Full + Binary\"][\"Accuracy\"].values[0]\n",
    "sparse_binary = threshold_results_df[threshold_results_df[\"Scenario\"] == \"B: Sparse + Binary\"][\"Accuracy\"].values[0]\n",
    "full_weighted = threshold_results_df[threshold_results_df[\"Scenario\"] == \"C: Full + Weighted\"][\"Accuracy\"].values[0]\n",
    "sparse_weighted = threshold_results_df[threshold_results_df[\"Scenario\"] == \"D: Sparse + Weighted\"][\"Accuracy\"].values[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EFFECT ANALYSIS (Threshold-based, 60% retention)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Baseline (A):              {baseline:.2%}\")\n",
    "print(f\"Structure effect (B - A):  {sparse_binary - baseline:+.2%}\")\n",
    "print(f\"Weighting effect (C - A):  {full_weighted - baseline:+.2%}\")\n",
    "print(f\"Combined effect (D - A):   {sparse_weighted - baseline:+.2%}\")\n",
    "print(f\"Interaction (D-B-C+A):     {sparse_weighted - sparse_binary - full_weighted + baseline:+.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143e998",
   "metadata": {},
   "source": [
    "### 2.2 Effect Calculation\n",
    "\n",
    "To isolate specific contributions, we calculate:\n",
    "*   **Structure Effect** = B - A (Impact of metric backbone sparsification)\n",
    "*   **Weighting Effect** = C - A (Impact of re-weighting edges)\n",
    "*   **Combined Effect** = D - A (Impact of doing both)\n",
    "*   **Interaction** = D - B - C + A (Synergy between structure and weighting)\n",
    "\n",
    "**Interpretation of Interaction:**\n",
    "- If **positive**: The two interventions are **synergistic** (combined > sum of parts)\n",
    "- If **zero**: The effects are **purely additive** (combined = sum of parts)\n",
    "- If **negative**: There is **interference** (combined < sum of parts)\n",
    "\n",
    "We will now calculate these values to quantify the impact of each intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146cb4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare effect analysis between threshold and metric backbone\n",
    "print(\"=\" * 70)\n",
    "print(\"EFFECT COMPARISON: Threshold vs Metric Backbone\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, df in [(\"Threshold (60%)\", threshold_results_df), (\"Metric Backbone\", backbone_results_df)]:\n",
    "    baseline = df[df[\"Scenario\"] == \"A: Full + Binary\"][\"Accuracy\"].values[0]\n",
    "    sparse_binary = df[df[\"Scenario\"] == \"B: Sparse + Binary\"][\"Accuracy\"].values[0]\n",
    "    full_weighted = df[df[\"Scenario\"] == \"C: Full + Weighted\"][\"Accuracy\"].values[0]\n",
    "    sparse_weighted = df[df[\"Scenario\"] == \"D: Sparse + Weighted\"][\"Accuracy\"].values[0]\n",
    "    retention = df[\"ActualRetention\"].mean()\n",
    "    \n",
    "    print(f\"\\n{name} (Retention: {retention:.1%}):\")\n",
    "    print(f\"  Baseline (A):              {baseline:.2%}\")\n",
    "    print(f\"  Structure effect (B - A):  {sparse_binary - baseline:+.2%}\")\n",
    "    print(f\"  Weighting effect (C - A):  {full_weighted - baseline:+.2%}\")\n",
    "    print(f\"  Combined effect (D - A):   {sparse_weighted - baseline:+.2%}\")\n",
    "    print(f\"  Interaction (D-B-C+A):     {sparse_weighted - sparse_binary - full_weighted + baseline:+.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36ad21",
   "metadata": {},
   "source": [
    "### 2.3 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison between threshold and metric backbone\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = [\"#2ecc71\", \"#3498db\", \"#e74c3c\", \"#9b59b6\"]\n",
    "\n",
    "for ax, (name, df) in zip(axes, [(\"Threshold (60%)\", threshold_results_df), (\"Metric Backbone\", backbone_results_df)]):\n",
    "    scenarios = df[\"Scenario\"].tolist()\n",
    "    accuracies = df[\"Accuracy\"].tolist()\n",
    "    retention = df[\"ActualRetention\"].mean()\n",
    "    baseline_acc = df[df[\"Scenario\"] == \"A: Full + Binary\"][\"Accuracy\"].values[0]\n",
    "    \n",
    "    bars = ax.bar(range(4), accuracies, color=colors, edgecolor=\"black\", linewidth=1.5)\n",
    "    ax.axhline(baseline_acc, color=\"gray\", linestyle=\"--\", linewidth=1, label=\"Baseline\")\n",
    "    \n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
    "                f\"{acc:.1%}\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\")\n",
    "    \n",
    "    ax.set_xticks(range(4))\n",
    "    ax.set_xticklabels([\"A\", \"B\", \"C\", \"D\"])\n",
    "    ax.set_ylabel(\"Test Accuracy\", fontsize=11)\n",
    "    ax.set_title(f\"{name}\\n(Retention: {retention:.1%})\", fontsize=12)\n",
    "    ax.set_ylim(0.5, 0.95)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Ablation Study: Cora + GCN + Jaccard\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1a7d4",
   "metadata": {},
   "source": [
    "## 3. Multi-Configuration Study\n",
    "\n",
    "Run comprehensive ablation studies comparing:\n",
    "- **Sparsification methods**: Threshold-based vs Metric Backbone\n",
    "- **Edge metrics**: Jaccard, Adamic-Adar, Approximate Effective Resistance\n",
    "- **Retention ratios**: 10% to 100% (for threshold-based)\n",
    "- **Seeds**: 42, 123, 456 (for statistical significance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for comprehensive experiments\n",
    "METRICS = [\"jaccard\", \"approx_er\", \"adamic_adar\"]\n",
    "RETENTION_RATIOS = [0.3, 0.5, 0.7, 0.9]  # For threshold-based\n",
    "SEEDS = [42, 123, 456]\n",
    "MODEL = \"gcn\"\n",
    "\n",
    "all_results_list = []\n",
    "\n",
    "# Run experiments on ALL datasets\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    \n",
    "    ds = datasets[dataset_name]\n",
    "    study = AblationStudy(\n",
    "        data=ds[\"data\"],\n",
    "        num_features=ds[\"num_features\"],\n",
    "        num_classes=ds[\"num_classes\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    # Run threshold-based experiments\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"THRESHOLD-BASED SPARSIFICATION - {dataset_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    threshold_df = study.run_multi_config_study(\n",
    "        model_names=[MODEL],\n",
    "        metrics=METRICS,\n",
    "        retention_ratios=RETENTION_RATIOS,\n",
    "        hidden_channels=64,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "        seeds=SEEDS,\n",
    "        use_metric_backbone=False,\n",
    "    )\n",
    "    threshold_df[\"SparsificationType\"] = \"Threshold\"\n",
    "    threshold_df[\"Dataset\"] = dataset_name\n",
    "    all_results_list.append(threshold_df)\n",
    "    \n",
    "    # Run metric backbone experiments\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"METRIC BACKBONE SPARSIFICATION - {dataset_name.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    backbone_df = study.run_multi_config_study(\n",
    "        model_names=[MODEL],\n",
    "        metrics=METRICS,\n",
    "        retention_ratios=[1.0],  # Placeholder, ignored when use_metric_backbone=True\n",
    "        hidden_channels=64,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "        seeds=SEEDS,\n",
    "        use_metric_backbone=True,\n",
    "    )\n",
    "    backbone_df[\"SparsificationType\"] = \"MetricBackbone\"\n",
    "    backbone_df[\"Dataset\"] = dataset_name\n",
    "    all_results_list.append(backbone_df)\n",
    "    \n",
    "    print(f\"Completed {dataset_name}: {len(threshold_df) + len(backbone_df)} experiments\")\n",
    "\n",
    "# Combine all results\n",
    "all_results_df = pd.concat(all_results_list, ignore_index=True)\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"TOTAL EXPERIMENTS COMPLETED: {len(all_results_df)}\")\n",
    "print(f\"Datasets: {all_results_df['Dataset'].unique().tolist()}\")\n",
    "print(f\"Columns: {all_results_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b140b56",
   "metadata": {},
   "source": [
    "### 3.1 Accuracy vs Retention Ratio (with Metric Backbone Reference)\n",
    "\n",
    "Plot accuracy across retention ratios for threshold-based methods, with metric backbone as a horizontal reference line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fe8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by computing mean and std across seeds\n",
    "threshold_df = all_results_df[all_results_df[\"SparsificationType\"] == \"Threshold\"]\n",
    "backbone_df = all_results_df[all_results_df[\"SparsificationType\"] == \"MetricBackbone\"]\n",
    "\n",
    "# Group threshold results\n",
    "threshold_agg = threshold_df.groupby([\"Dataset\", \"Metric\", \"Retention\", \"Scenario\"]).agg({\n",
    "    \"Accuracy\": [\"mean\", \"std\"],\n",
    "    \"ActualRetention\": \"mean\",\n",
    "}).reset_index()\n",
    "threshold_agg.columns = [\"Dataset\", \"Metric\", \"Retention\", \"Scenario\", \"Accuracy_mean\", \"Accuracy_std\", \"ActualRetention\"]\n",
    "\n",
    "# Group backbone results\n",
    "backbone_agg = backbone_df.groupby([\"Dataset\", \"Metric\", \"Scenario\"]).agg({\n",
    "    \"Accuracy\": [\"mean\", \"std\"],\n",
    "    \"ActualRetention\": \"mean\",\n",
    "}).reset_index()\n",
    "backbone_agg.columns = [\"Dataset\", \"Metric\", \"Scenario\", \"Accuracy_mean\", \"Accuracy_std\", \"ActualRetention\"]\n",
    "\n",
    "# Plot: Accuracy vs Retention for Scenario D by Dataset\n",
    "scenario_d = \"D: Sparse + Weighted\"\n",
    "colors = {\"jaccard\": \"#2ecc71\", \"approx_er\": \"#e74c3c\", \"adamic_adar\": \"#3498db\"}\n",
    "n_datasets = len(DATASET_NAMES)\n",
    "n_metrics = len(METRICS)\n",
    "\n",
    "fig, axes = plt.subplots(n_datasets, n_metrics, figsize=(5 * n_metrics, 4 * n_datasets))\n",
    "\n",
    "for i, dataset_name in enumerate(DATASET_NAMES):\n",
    "    for j, metric in enumerate(METRICS):\n",
    "        ax = axes[i, j] if n_datasets > 1 else axes[j]\n",
    "        \n",
    "        # Threshold data\n",
    "        mask = (threshold_agg[\"Dataset\"] == dataset_name) & \\\n",
    "               (threshold_agg[\"Metric\"] == metric) & \\\n",
    "               (threshold_agg[\"Scenario\"] == scenario_d)\n",
    "        metric_data = threshold_agg[mask]\n",
    "        \n",
    "        if len(metric_data) > 0:\n",
    "            ax.errorbar(\n",
    "                metric_data[\"Retention\"], \n",
    "                metric_data[\"Accuracy_mean\"],\n",
    "                yerr=metric_data[\"Accuracy_std\"],\n",
    "                marker=\"o\", \n",
    "                label=\"Threshold\",\n",
    "                color=colors.get(metric, \"gray\"),\n",
    "                capsize=3,\n",
    "            )\n",
    "        \n",
    "        # Metric backbone reference\n",
    "        bb_mask = (backbone_agg[\"Dataset\"] == dataset_name) & \\\n",
    "                  (backbone_agg[\"Metric\"] == metric) & \\\n",
    "                  (backbone_agg[\"Scenario\"] == scenario_d)\n",
    "        backbone_data = backbone_agg[bb_mask]\n",
    "        \n",
    "        if len(backbone_data) > 0:\n",
    "            bb_acc = backbone_data[\"Accuracy_mean\"].values[0]\n",
    "            bb_ret = backbone_data[\"ActualRetention\"].values[0]\n",
    "            bb_std = backbone_data[\"Accuracy_std\"].values[0]\n",
    "            ax.axhline(bb_acc, color=\"black\", linestyle=\"--\", linewidth=2, \n",
    "                      label=f\"Backbone ({bb_ret:.0%})\")\n",
    "            ax.axhspan(bb_acc - bb_std, bb_acc + bb_std, alpha=0.2, color=\"gray\")\n",
    "        \n",
    "        ax.set_xlabel(\"Retention Ratio\")\n",
    "        ax.set_ylabel(\"Accuracy\")\n",
    "        ax.set_title(f\"{dataset_name.upper()} - {metric.replace('_', ' ').title()}\")\n",
    "        ax.legend(loc=\"lower right\", fontsize=8)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_ylim(0.3, 0.95)\n",
    "\n",
    "fig.suptitle(f\"Scenario D: Threshold vs Metric Backbone (All Datasets)\\nModel: {MODEL}, Seeds: {SEEDS}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9eedd5",
   "metadata": {},
   "source": [
    "## 4. Cross-Model Comparison\n",
    "\n",
    "Compare ablation results across different GNN architectures (GCN, SAGE, GAT) at a fixed retention ratio (70%), comparing both threshold-based and metric backbone approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbe1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-model comparison on ALL datasets\n",
    "MODELS = [\"gcn\", \"sage\", \"gat\"]\n",
    "FIXED_RETENTION = 0.7\n",
    "\n",
    "cross_model_list = []\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"# CROSS-MODEL: {dataset_name.upper()}\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    \n",
    "    ds = datasets[dataset_name]\n",
    "    study = AblationStudy(\n",
    "        data=ds[\"data\"],\n",
    "        num_features=ds[\"num_features\"],\n",
    "        num_classes=ds[\"num_classes\"],\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    \n",
    "    # Threshold-based for all models\n",
    "    print(f\"\\nThreshold-based ({FIXED_RETENTION:.0%} retention)...\")\n",
    "    cross_threshold = study.run_multi_config_study(\n",
    "        model_names=MODELS,\n",
    "        metrics=METRICS,\n",
    "        retention_ratios=[FIXED_RETENTION],\n",
    "        hidden_channels=64,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "        seeds=SEEDS,\n",
    "        use_metric_backbone=False,\n",
    "    )\n",
    "    cross_threshold[\"SparsificationType\"] = \"Threshold\"\n",
    "    cross_threshold[\"Dataset\"] = dataset_name\n",
    "    cross_model_list.append(cross_threshold)\n",
    "    \n",
    "    # Metric backbone for all models\n",
    "    print(f\"Metric backbone...\")\n",
    "    cross_backbone = study.run_multi_config_study(\n",
    "        model_names=MODELS,\n",
    "        metrics=METRICS,\n",
    "        retention_ratios=[1.0],\n",
    "        hidden_channels=64,\n",
    "        epochs=200,\n",
    "        patience=20,\n",
    "        seeds=SEEDS,\n",
    "        use_metric_backbone=True,\n",
    "    )\n",
    "    cross_backbone[\"SparsificationType\"] = \"MetricBackbone\"\n",
    "    cross_backbone[\"Dataset\"] = dataset_name\n",
    "    cross_model_list.append(cross_backbone)\n",
    "    \n",
    "    print(f\"Completed {dataset_name}\")\n",
    "\n",
    "# Combine\n",
    "cross_model_df = pd.concat(cross_model_list, ignore_index=True)\n",
    "print(f\"\\nTotal cross-model experiments: {len(cross_model_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8055ae",
   "metadata": {},
   "source": [
    "### 4.1 Comparative Analysis of Architectures\n",
    "\n",
    "Compare how GCN, GraphSAGE, and GAT perform under threshold-based vs metric backbone sparsification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9ef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate cross-model results\n",
    "cross_agg = cross_model_df.groupby([\"Dataset\", \"Model\", \"Metric\", \"Scenario\", \"SparsificationType\"]).agg({\n",
    "    \"Accuracy\": [\"mean\", \"std\"],\n",
    "    \"TrainSec\": \"mean\",\n",
    "    \"SparsifySec\": \"mean\",\n",
    "    \"ActualRetention\": \"mean\",\n",
    "}).reset_index()\n",
    "cross_agg.columns = [\"Dataset\", \"Model\", \"Metric\", \"Scenario\", \"SparsificationType\", \n",
    "                     \"Accuracy_mean\", \"Accuracy_std\", \"TrainSec\", \"SparsifySec\", \"ActualRetention\"]\n",
    "\n",
    "# Print summary table for Scenario D by dataset\n",
    "print(\"=\" * 100)\n",
    "print(\"CROSS-MODEL SUMMARY: Scenario D (Sparse + Weighted) - All Datasets\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for dataset_name in DATASET_NAMES:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    ds_data = cross_agg[(cross_agg[\"Dataset\"] == dataset_name) & (cross_agg[\"Scenario\"] == scenario_d)]\n",
    "    if len(ds_data) > 0:\n",
    "        pivot = ds_data.pivot_table(\n",
    "            index=[\"Model\", \"Metric\"], \n",
    "            columns=\"SparsificationType\", \n",
    "            values=[\"Accuracy_mean\", \"ActualRetention\"],\n",
    "            aggfunc=\"first\"\n",
    "        )\n",
    "        print(pivot.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba944233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-model comparison: compact visualization for all datasets\n",
    "n_datasets = len(DATASET_NAMES)\n",
    "n_cols = 4\n",
    "n_rows = (n_datasets + n_cols - 1) // n_cols  # Ceiling division\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "scenario_d_agg = cross_agg[cross_agg[\"Scenario\"] == scenario_d]\n",
    "bar_width = 0.35\n",
    "x = np.arange(len(MODELS))\n",
    "\n",
    "for idx, dataset_name in enumerate(DATASET_NAMES):\n",
    "    ax = axes[idx]\n",
    "    ds_data = scenario_d_agg[scenario_d_agg[\"Dataset\"] == dataset_name]\n",
    "    \n",
    "    # Average across metrics for cleaner visualization\n",
    "    threshold_by_model = ds_data[ds_data[\"SparsificationType\"] == \"Threshold\"].groupby(\"Model\").agg({\n",
    "        \"Accuracy_mean\": \"mean\",\n",
    "        \"Accuracy_std\": \"mean\",\n",
    "    }).reindex(MODELS)\n",
    "    \n",
    "    backbone_by_model = ds_data[ds_data[\"SparsificationType\"] == \"MetricBackbone\"].groupby(\"Model\").agg({\n",
    "        \"Accuracy_mean\": \"mean\",\n",
    "        \"Accuracy_std\": \"mean\",\n",
    "    }).reindex(MODELS)\n",
    "    \n",
    "    bars1 = ax.bar(x - bar_width/2, threshold_by_model[\"Accuracy_mean\"], bar_width, \n",
    "                   yerr=threshold_by_model[\"Accuracy_std\"],\n",
    "                   label=\"Threshold (70%)\", color=\"#3498db\", capsize=3, alpha=0.8)\n",
    "    bars2 = ax.bar(x + bar_width/2, backbone_by_model[\"Accuracy_mean\"], bar_width,\n",
    "                   yerr=backbone_by_model[\"Accuracy_std\"],\n",
    "                   label=\"Metric Backbone\", color=\"#2ecc71\", capsize=3, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(f\"{dataset_name.upper()}\", fontsize=10)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([m.upper() for m in MODELS], fontsize=8)\n",
    "    ax.legend(loc=\"lower right\", fontsize=7)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    ax.set_ylim(0.1, 0.95)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(n_datasets, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(f\"Cross-Model Comparison: Threshold vs Metric Backbone\\n(Scenario D, Averaged Across Metrics, Seeds: {SEEDS})\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e28ef",
   "metadata": {},
   "source": [
    "## 5. Save Results and Summary\n",
    "\n",
    "Save all results for analysis in notebook 05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqxk9opkjfq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all results from multi-config and cross-model studies\n",
    "final_results_df = pd.concat([all_results_df, cross_model_df], ignore_index=True)\n",
    "\n",
    "# Remove duplicates (same config may appear in both studies)\n",
    "final_results_df = final_results_df.drop_duplicates(\n",
    "    subset=[\"Dataset\", \"Model\", \"Metric\", \"Retention\", \"Scenario\", \"SparsificationType\", \"Seed\"]\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "output_path = Path(\"../data/ablation_results_comprehensive.csv\")\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "final_results_df.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(final_results_df)} experiment results to {output_path}\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total experiments: {len(final_results_df)}\")\n",
    "print(f\"Datasets: {sorted(final_results_df['Dataset'].unique().tolist())}\")\n",
    "print(f\"Sparsification types: {final_results_df['SparsificationType'].unique().tolist()}\")\n",
    "print(f\"Models tested: {sorted(final_results_df['Model'].unique().tolist())}\")\n",
    "print(f\"Metrics tested: {sorted(final_results_df['Metric'].unique().tolist())}\")\n",
    "print(f\"Seeds used: {sorted(final_results_df['Seed'].unique().tolist())}\")\n",
    "\n",
    "# Summary table: Best accuracy per dataset and method\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST ACCURACY BY DATASET & SPARSIFICATION TYPE (Scenario D)\")\n",
    "print(\"=\" * 70)\n",
    "best_summary = final_results_df[final_results_df[\"Scenario\"] == \"D: Sparse + Weighted\"].groupby(\n",
    "    [\"Dataset\", \"SparsificationType\"]\n",
    ").agg({\n",
    "    \"Accuracy\": [\"mean\", \"std\", \"max\"],\n",
    "    \"ActualRetention\": \"mean\",\n",
    "    \"TrainSec\": \"mean\",\n",
    "}).round(4)\n",
    "print(best_summary.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
